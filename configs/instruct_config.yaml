data:
  mode: "instruct"
  data: 
    mixed: "data/sft_merge.jsonl"
  pad_to_max: True
  sequence_sample_mode: "truncation"
  concat_multiple_sequence: False
  num_sequences: 50
  seq_length: 1024
  tokenizer_model_path: "configs/tokenizer_models/vocab_32k_gpt2_moe.model"
  split_by_shard: False
train:
  train_batch_size: 8
  # 1B token for 1 epoch, 5epoch
  num_training_steps: 1000000
  num_warmup_steps: 500
  initializer_range: 1.0e-2
  lr: 2.0e-4
  weight_decay: 1.0e-1
  ckpt: "ckpt/vocab_32k_gpt2_moe/"
  train_num_workers_4_dataloader: 16
  gradient_accumulation_steps: 5
  prefetch_factor: 100
  train_and_eval: False
  gradient_checkpointing_enable: False
  use_lora: False
# global step
log_interval: 10
eval_interval: 500
save_interval: 5000
work_dir: "ckpt/vocab_32k_gpt2_moe_instruction"
project_name: "vocab_32k_gpt2_moe Instruction"