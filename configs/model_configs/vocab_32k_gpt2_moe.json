{
  "architectures": [
    "vocab_32k_GPT2_MOELMHeadModel"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 3,
  "vocab_size":32000,
  "attention_bias": false,
  "attention_dropout": 0.0,
  "first_k_dense_replace": 0,
  "moe_layer_freq": 1,
  "n_routed_experts": 8,
  "n_shared_experts": null,
  "norm_topk_prob": true,
  "num_experts_per_tok": 2,
  "use_cache": true,
  "n_layer": 12
}
